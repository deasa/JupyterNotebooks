{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "directory_path = '/Users/brendanashton/dev/go/src/github.com/deasa/pinkbike_crawler/runs'\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "dataset = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "dataset.head()\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns=['Title', 'Frame Size', 'Reason for Review', 'URL', 'Original Currency'])\n",
    "dataset.head()\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the year - some are missing and some are strangely high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_year = pd.Timestamp.now().year\n",
    "min_year = 2000\n",
    "max_year = pd.Timestamp.today().year \n",
    "\n",
    "# Calculate the mean (average) year within the bounds\n",
    "mean_year = dataset[(dataset['Year'] >= min_year) & (dataset['Year'] <= max_year)]['Year'].mean()\n",
    "\n",
    "# Replace out-of-bounds low years with NaN\n",
    "dataset.loc[(dataset['Year'] < min_year), 'Year'] = np.nan\n",
    "\n",
    "# Replace out-of-bounds years with the mean (Corrected line)\n",
    "dataset.loc[(dataset['Year'] > max_year), 'Year'] = mean_year\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract numbers from front and rear travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract numerical values using regular expressions\n",
    "dataset['Rear Travel'] = dataset['Rear Travel'].astype(str).str.extract('(\\d+)', expand=False)\n",
    "\n",
    "# Convert to numeric, setting failed conversions to NaN\n",
    "dataset['Rear Travel'] = pd.to_numeric(dataset['Rear Travel'], errors='coerce')\n",
    "\n",
    "dataset['Rear Travel'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Front Travel'] = dataset['Front Travel'].astype(str).str.extract('(\\d+)', expand=False)\n",
    "dataset['Front Travel'] = pd.to_numeric(dataset['Front Travel'], errors='coerce')\n",
    "\n",
    "dataset['Front Travel'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace NoManufacturer and NoModelFound with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.replace('NoModelFound', np.nan, inplace=True)\n",
    "dataset.replace('NoManufacturer', np.nan, inplace=True)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove any electric hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Model'] = dataset['Model'].astype(str).str.replace(r'.*electric.*', \"NaN\", case=False, regex=True)\n",
    "dataset.replace('nan', np.nan, inplace=True)\n",
    "dataset.replace('NaN', np.nan, inplace=True)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build up manual predictions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame for manual predictions\n",
    "manual_predictions = pd.DataFrame({\n",
    "    'Year': [2019, 2021, 2021, 2022],\n",
    "    'Manufacturer': ['Specialized', 'Canyon', 'Ibis', 'Specialized'],\n",
    "    'Model': ['Stumpjumper', 'Spectral', 'Ripmo AF', 'Status 140'],\n",
    "    'USD Price': [2000, 2500, 2000, 2000],\n",
    "    'Condition': ['Good - Used, Mechanically Sound', 'Good - Used, Mechanically Sound', 'Good - Used, Mechanically Sound', 'Good - Used, Mechanically Sound'],\n",
    "    'Wheel Size': ['29', '29', '29', '29'],\n",
    "    'Front Travel': [150, 160, 160, 140],\n",
    "    'Rear Travel': [140, 150, 147, 140],\n",
    "    'Material': ['Carbon Fiber', 'Carbon Fiber', 'Aluminum', 'Aluminum'],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Model - it adds too much noise to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns=['Model'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_predictions = manual_predictions.drop(columns=['Model'])\n",
    "manual_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original DataFrame shape: {dataset.shape}\")\n",
    "\n",
    "# Drop duplicate rows, keeping the first occurrence\n",
    "dataset = dataset.drop_duplicates(keep='first')\n",
    "\n",
    "print(f\"Deduplicated DataFrame shape: {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop all NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original DataFrame shape: {dataset.shape}\")\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Print the shape of the original and deduplicated DataFrames\n",
    "\n",
    "print(f\"No NA DataFrame shape: {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute an age column, drop the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def convert_age(X):\n",
    "    if 'Year' not in X.columns:\n",
    "        raise KeyError(\"The DataFrame does not contain a 'Year' column.\")\n",
    "    \n",
    "    # Make a copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "    X = X.copy()\n",
    "    \n",
    "    current_year = datetime.now().year\n",
    "    X.loc[:, 'Age'] = current_year - X['Year']\n",
    "    return X\n",
    "\n",
    "dataset = convert_age(dataset)\n",
    "# Drop the 'Year' column\n",
    "dataset = dataset.drop(columns='Year')\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_predictions = convert_age(manual_predictions)\n",
    "manual_predictions = manual_predictions.drop(columns='Year')\n",
    "manual_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put them in categories based on their travel numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'Category' that will categorize the listings by the rear travel.\n",
    "def categorize_travel(X):\n",
    "    if 'Rear Travel' not in X.columns:\n",
    "        raise KeyError(\"The DataFrame does not contain a 'Rear Travel' column.\")\n",
    "    \n",
    "    # Make a copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Create a new column 'Category' based on the 'Rear Travel' column\n",
    "    X.loc[(X['Rear Travel'] < 0), 'Category'] = np.nan\n",
    "    X.loc[(X['Rear Travel'] > 210), 'Category'] = np.nan\n",
    "    X.loc[(X['Rear Travel'] == 0), 'Category'] = 'Hardtail'\n",
    "    X.loc[(X['Rear Travel'] >= 0) & (X['Rear Travel'] < 120), 'Category'] = 'Short Travel'\n",
    "    X.loc[(X['Rear Travel'] >= 120) & (X['Rear Travel'] <= 150), 'Category'] = 'Mid Travel'\n",
    "    X.loc[(X['Rear Travel'] > 150) & (X['Rear Travel'] <= 210), 'Category'] = 'Long Travel'\n",
    "    return X\n",
    "\n",
    "print(f\"Original DataFrame shape: {dataset.shape}\")\n",
    "dataset = categorize_travel(dataset)\n",
    "dataset = dataset.dropna()\n",
    "print(f\"DataFrame shape after categorization: {dataset.shape}\")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_predictions = categorize_travel(manual_predictions)\n",
    "manual_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop now-irrelevant front travel and rear travel columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns=['Rear Travel', 'Front Travel'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_predictions = manual_predictions.drop(columns=['Rear Travel', 'Front Travel'])\n",
    "manual_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorder columns so the target variable is at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[[col for col in dataset.columns if col != 'USD Price'] + ['USD Price']]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_predictions = manual_predictions[[col for col in manual_predictions.columns if col != 'USD Price'] + ['USD Price']]\n",
    "manual_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into features and a target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, :-1]\n",
    "Y = dataset.iloc[:, -1]\n",
    "X_manual = manual_predictions.iloc[:, :-1]\n",
    "Y_manual = manual_predictions.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the data with one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(X):\n",
    "    for column in ['Manufacturer', 'Condition', 'Wheel Size', 'Material', 'Category']:\n",
    "        insert_loc = X.columns.get_loc(column)\n",
    "        X = pd.concat([X.iloc[:,:insert_loc], pd.get_dummies(X.loc[:, [column]]), X.iloc[:,insert_loc+1:]], axis=1)\n",
    "    return X\n",
    "\n",
    "# temporarily join the X and X_manual DataFrames to encode the categorical variables\n",
    "X = pd.concat([X, X_manual], ignore_index=True)\n",
    "X = np.array(encode_data(X.copy()))\n",
    "\n",
    "# Split the encoded data back into the original X and X_manual DataFrames\n",
    "X, X_manual = X[:len(dataset)], X[len(dataset):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into a training set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "# split_index = int(len(X_transformed) * 0.8)\n",
    "# X_train = X_transformed[:split_index]\n",
    "# X_test = X_transformed[split_index:]\n",
    "# Y_train = Y[:split_index]\n",
    "# Y_test = Y[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)\n",
    "# print(dataset.iloc[split_index:, -2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply feature scaling\n",
    "We don't actually have to apply feature scaling for multiple linear regression because the coefficient for each independent variable will auto adjust according to its significance and its raw values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = regressor.predict(X_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_array = Y_test.to_numpy().reshape(len(Y_test),1)\n",
    "Y_pred_array = Y_pred.reshape(len(Y_pred),1)\n",
    "comparisons = np.concatenate((Y_pred_array, Y_test_array),1)\n",
    "print(comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Compute mean absolute error (MAE)\n",
    "mae = mean_absolute_error(Y_test_array, Y_pred_array)\n",
    "\n",
    "# Compute mean squared error (MSE)\n",
    "mse = mean_squared_error(Y_test_array, Y_pred_array)\n",
    "\n",
    "# Compute root mean squared error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "mae, mse, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the prices of the manual predictions\n",
    "Y_manual_pred = regressor.predict(X_manual)\n",
    "\n",
    "print('Y_manual_pred:', Y_manual_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
